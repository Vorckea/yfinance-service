name: Test

permissions:
  contents: read

on:
  workflow_call:
    inputs:
      python-version:
        required: true
        type: string
        description: 'Python version to use for testing'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5

      - name: Setup & Install
        uses: ./.github/actions/setup-python-deps
        with:
          python_version: ${{ inputs.python-version }}
          development: true

      - name: Run tests
        run: > 
          poetry run pytest 
          --maxfail=1 --disable-warnings --tb=short
          --cov=app
          --cov-report=term-missing 
          --cov-report=xml 
          --cov-report=html
          --junitxml=pytest-report.xml

      - name: Add pytest summary to job summary
        if: always()
        run: |
          python - <<'PY'
          import os, xml.etree.ElementTree as ET
          junit = 'pytest-report.xml'
          summary_path = os.environ.get('GITHUB_STEP_SUMMARY')
          if not summary_path:
              print("GITHUB_STEP_SUMMARY not available")
              raise SystemExit(0)
          try:
              tree = ET.parse(junit)
              root = tree.getroot()
          except Exception as e:
              open(summary_path, 'a').write(f"**Pytest report parsing failed:** {e}\n")
              raise SystemExit(0)
          total = failures = errors = skipped = 0
          failed_tests = []
          for ts in root.findall('.//testsuite'):
              total += int(ts.attrib.get('tests', 0))
              failures += int(ts.attrib.get('failures', 0))
              errors += int(ts.attrib.get('errors', 0))
              skipped += int(ts.attrib.get('skipped', 0))
              for tc in ts.findall('testcase'):
                  for f in tc.findall('failure') + tc.findall('error'):
                      msg = (f.attrib.get('message') or '').splitlines()[0]
                      failed_tests.append(f"- **{tc.attrib.get('classname','')}.{tc.attrib.get('name','')}** â€” {msg}")
          md = [
              "## Pytest summary",
              f"- total: {total}",
              f"- failures: {failures}",
              f"- errors: {errors}",
              f"- skipped: {skipped}",
              ""
          ]
          if failed_tests:
              md.append("### Failed tests")
              md.extend(failed_tests[:20])
              if len(failed_tests) > 20:
                  md.append(f"...and {len(failed_tests)-20} more")
          open(summary_path, 'a').write('\\n'.join(md))
          PY

      - name: Upload Pytest JUnit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest-report
          path: pytest-report.xml

      - name: Upload Coverage XML
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-xml
          path: coverage.xml

      - name: Upload Coverage HTML
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-html
          path: htmlcov/